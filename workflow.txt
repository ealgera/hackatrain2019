<cd naar directory>

python3 -m venv scraping

source scraping/bin/activate

pip install requests

pip install beautifulsoup4

code .

import packages
  from bs4 import BeautifulSoup 
  import requests

maak url
  url = "https://github.com/trending"

maak response met requests.get(url)
print de response --> html code 200 als het goed is.
  response = requests.get(url)
  print(response)

haal de text uit de response (response.text)
print de text
  my_html = response.text
  
maak een soup-object: soup = BeautifulSoup(my_html, 'html.parser')
print soup
print soup.prettify  
  soup = BeautifulSoup(my_html, 'html.parser')
  print(soup)
  print(soup.prettify)

bewaar soup text in bestand om structuur te kunnen laten zien
  scrape-test.py > html-txt.txt
  
Doe nu ook een 'inspect' van de pagina en laat de structuur zien

Laat zien welke (eerste) div je nu vindt. In deze div staan de verwijzingen naar de volgende pagina's.
buyers = soup.find("div")

Laat de div.text zien (alleen 1, 2, 3, 4, 5)
Laat de div ahref zien (alleen verwijzing naar pagina 2)
Doe nu een find_all van de a-href in buyers.
Print, via een for-loop, alle hrefs.
Voeg de eerste pagina toe aan de list.
Bewaar de pagina hrefs in een list (list comprehension), voor later
Laat nu opnieuw de pagina-inspect zien 

Zoek de div met title="buyer-info"
Zoek de div met title="buyer-name"
Laat de verschillen zien.

Zoek div buyer_name in buyer_info
Zoek span buyer_price in buyer_info
print beide gegevens

Dan een find_all zoals buyer_info. Print deze gegevens
Dan hetzelfde, maar dan via een loop: for buyer in soup.find_all ...
En zoek binnen de loop naar buyer_name, buyer_price. Print deze.

Maak een lege lijst voor de loop (buyers)
Vul de lijst buyers met buyers_name en buyers_price (list in list)
Laat de buyers lijst zien.

Maak een def van de for-loop over buyers. Param: bs-soup, return list

maak een loop over de page_list (for url in page_list)
	print(parsing... url)
	response
	response.text
	soup element
	concatenate all_buyers met def output

import csv
open buyers.csv writable binnen with context
schrijf header row
schrijf(all_buyers)


Example 2

<cd naar directory>

python3 -m venv scraping

source scraping/bin/activate

pip install requests
pip install beautifulsoup4

code .

import packages
  from bs4 import BeautifulSoup 
  import requests

maak url
  url = "https://github.com/trending"

maak response met requests.get(url)
print de response --> html code 200 als het goed is.
  response = requests.get(url)
  print(response)

haal de text uit de response (response.text)
print de text
  my_html = response.text
  
maak een soup-object: soup = BeautifulSoup(my_html, 'html.parser')
print soup
print soup.prettify  
  soup = BeautifulSoup(my_html, 'html.parser')
  print(soup)
  print(soup.prettify)

bewaar soup text in bestand om structuur te kunnen laten zien
  scrape-test.py > html-txt.txt

Open website en laat met inspect de opbouw van de pagina zien.

Laat zien dat de ordered_list (ol) 'repo-list' alle elementen omvat

Doe een soup.find van ol met class 'repo-list' en laat deze zien
  repo_list = soup.find("ol", {"class": "repo-list"}) / soup.find("ol", "class_="repo-list")
  print(repo_list)
  
Doe een find binnen repo-list op de list met classe "col-12 d-block width-full py-4 border-bottom"
Laat dit repo-element zien
  repo_element = repo_list.find("li", {"class": "col-12 d-block width-full py-4 border-bottom"})
  print(repo_element)

Laat zien dat een find op de eerst "a" in repo-element een string met author en project laat zien (gescheiden door een '/')
  repo_author = repo_element.find("a")
  print(repo_author.text) / print(repo_author.get_text())

Doe dan een split('/') op find "a" in repo-element om author en project te scheiden
  repo_author, repo_name = repo_element.find("a").text.split(" / ")

Doe nu een find_all op de "li" met class "col-12 d-block width-full py-4 border-bottom" om alle elemente te vinden
print author en projectnaam
  for repo_element in repo_list.find_all("li", "col-12 d-block width-full py-4 border-bottom"):
    print(repo_element)
    print("-" * 25)

Nu met author en name:
    repo_author = repo_element.find("a").text

Split en daarna strip laten zien
    repo_author, repo_name = repo_element.find("a").text.split(" / ")
    repo_author, repo_name = repo_element.find("a").text.strip().split(" / ")

Laat op de pagina met inspect zien waar het aantal sterren te vinden is. ("a", {"class": "muted-link d-inline-block mr-3"))
print author, name en stars, allen met een strip()
    repo_stars = repo_element.find("a", {"class": "muted-link d-inline-block mr-3"}).text.strip()
    print(repo_author, "-", repo_name, "-", repo_stars)
    print("-" * 25   )

Zoek de link in repo_element
   repo_link = repo_element.find("a").get("href")
En voeg GitHub toe:
   repo_link = "https://github.com" + repo_element.find("a").get("href")

Voeg alles toe aan een lege list (repo_trends) en print deze
  repo_trends = []
  for ...
    ...
    repo_trends.append([repo_author, repo_name, repo_stars, repo_link])   
print de lijst

Probeer de lijst te sorteren maar laat zien dat het een alfanumerieke sort is (op stars)
   repo_trends = sorted(repo_trends, key=lambda x: x[2], reverse=True)

Zet repo_stars om naar numeriek om goed te kunnen sorteren (eerst zonder replace: foutmelding!)
Doe een replace van ',' naar <none> en cast naar int()
Laat de lijst opnieuw zien.
Sorteer de lijst nu goed op de derde kolom (stars))
    repo_trends.append([repo_author, repo_name, int(repo_stars.replace(",","")), repo_link])

Laat zien dat je op de eerste find "a" in repo-element ook de href kunt pakken (get("href")): repo_link
Voeg "https://github.com" toe aan repo_link
Voeg repo_link toe aan de lijst repo_trends

print ieder element van de repo_trends

Schrijf de lijst weg naar een CSV bestand
   with open("trending.csv","w") as csv_file:
      wr = csv.writer(csv_file)
      wr.writerows(repo_trends)

Open de CSV in pandas
   pip install pandas
   
   import pandas as pd
   df = pd.read_csv("trending.csv", names=["Author", "Name", "Stars", "Link"])
   print(type(df))

Print de Stars kolom
   print(df.Stars)
   
Plot de Stars als bar-graph
   pip install matplotlib
   
   import matplotlib.pyplot as plt
   
   df.Stars.plot(kind="bar")
   plt.show()









