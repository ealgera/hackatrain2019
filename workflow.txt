<cd naar directory>

python3 -m venv scraping

source scraping/bin/activate

pip install requests

pip install beautifulsoup4

code .

import packages
  from bs4 import BeautifulSoup 
  import requests

maak url
  url = "https://github.com/trending"

maak response met requests.get(url)
print de response --> html code 200 als het goed is.
  response = requests.get(url)
  print(response)

haal de text uit de response (response.text)
print de text
  my_html = response.text
  
maak een soup-object: soup = BeautifulSoup(my_html, 'html.parser')
print soup
print soup.prettify  
  soup = BeautifulSoup(my_html, 'html.parser')
  print(soup)
  print(soup.prettify)

bewaar soup text in bestand om structuur te kunnen laten zien
Doe nu ook een 'inspect' van de pagina en laat de structuur zien
buyers = soup.find("div"): laat zien welke (eerste) div je nu vindt. In deze div staan de verwijzingen naar de volgende pagina's.
Laat de div.text zien (alleen 1, 2, 3, 4, 5)
Laat de div ahref zien (alleen verwijzing naar pagina 2)
Doe nu een find_all van de a-href in buyers.
Print, via een for-loop, alle hrefs.
Voeg de eerste pagina toe aan de list.
Bewaar de pagina hrefs in een list (list comprehension), voor later
Laat nu opnieuw de pagina-inspect zien 

Zoek de div met title="buyer-info"
Zoek de div met title="buyer-name"
Laat de verschillen zien.

Zoek div buyer_name in buyer_info
Zoek span buyer_price in buyer_info
print beide gegevens

Dan een find_all zoals buyer_info. Print deze gegevens
Dan hetzelfde, maar dan via een loop: for buyer in soup.find_all ...
En zoek binnen de loop naar buyer_name, buyer_price. Print deze.

Maak een lege lijst voor de loop (buyers)
Vul de lijst buyers met buyers_name en buyers_price (list in list)
Laat de buyers lijst zien.

Maak een def van de for-loop over buyers. Param: bs-soup, return list

maak een loop over de page_list (for url in page_list)
	print(parsing... url)
	response
	response.text
	soup element
	concatenate all_buyers met def output

import csv
open buyers.csv writable binnen with context
schrijf header row
schrijf(all_buyers)

Example 2
Open website en laat met inspect de opbouw van de pagina zien.
Laat zien dat de ordered_list (ol) 'repo-list' alle elementen omvat
Doe een soup.find van ol met class 'repo-list' en laat deze zien

Doe een find binnen repo-list op de list met classe "col-12 d-block width-full py-4 border-bottom"
Laat dit repo-element zien

Laat zien dat een find op de eerst "a" in repo-element een string met author en project laat zien (gescheiden door een '/')
Doe dan een split('/') op find "a" in repo-element om author en project te scheiden

Doe nu een find_all op de "li" met class "col-12 d-block width-full py-4 border-bottom" om alle elemente te vinden
print author en projectnaam

Laat op de pagina met inspect zien waar het aantal sterren te vinden is. ("a", {"class": "muted-link d-inline-block mr-3"))
print author, name en stars, allen met een strip()


print nu alles
Voeg alles toe aan een lege list (repo_trends) en print deze

Probeer de lijst te sorteren maar laat zien dat het een alfanumerieke sort is (op stars)
Doe een replace van ',' naar <none> en cast naar int()
Laat de lijst opnieuw zien.
Sorteer de lijst nu goed op de derde kolom (stars))

Laat zien dat je op de eerste find "a" in repo-element ook de href kunt pakken (get("href")): repo_link
Voeg "https://github.com" toe aan repo_link
Voeg repo_link toe aan de lijst repo_trends

print ieder element van de repo_trends








